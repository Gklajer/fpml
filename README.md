# Fundamental Principles of Machine Learning

Gitlab for the course Fundamental Principles of Machine Learning (FPML or TC0) in the CS Masters (M1 year), in the track *Artificial Intelligence* of the Paris Saclay University

The e-campus address is (for the quizz):

https://ecampus.paris-saclay.fr/course/view.php?id=92339#section-7

# IMPORTANT: PRE-REQUISITES !

**PRE1 and PRE2 as prerequisites, and PRE4 is strongly recommended**. Note that **PRE1 and PRE2 are mandatory, i.e. you must attend them to be allowed to follow TC0/FPML**, except if you can argue that you are already very fluent in statistics (PRE1) and linear algebra (PRE2) (in that case, e-mail me).
**This class** (or an equivalent) **is a prerequisite for almost all other [AI] classes**.

# Catch up for Nov 11th missed class

- To catch up on Friday 11th nov, we can try to find a date that suits everyone, here: https://www.when2meet.com/?17729512-WxKDI


# What we did in class

## session 1 - GD+Linear Regression (+organizational stuff)

During the first session, we did (i give the paths to files from the gitlab):

- read together "the rules": 0-organization-rules-intro/0-organization-rules.pdf
- The main Lecture material was: 1-Regressions: lecture1-ML-vocab+LinReg.pdf
- Still lecture material: 1-GradientDescent/Gradient Descent-demo-compute2gradients.ipynb  : I explained in detail GD and its pitfalls, in particular in 1D. We did not compute the 2D case but ou can do it at home.

In tutorial, we did (may depend slightly on the group you are in):

- 1-Regressions: TD-Regressions.pdf -> we did the 1.1.1, 1.1.2, maybe the 1.1.3 (it's not very different from those before if you understand how to vectorize everything),
- in terms of notebooks, you filled the TP1.1-LinearRegression-GradientDescent.ipynb (either skipping the naive version and going directly for trick of the ones, or not), some people did also the TP1.2-LinReg-d-dimensional.ipynb (which is very similar to TP1.1)

A few students did also attack the TP1.3-LinReg-3rd-order-Polynomial.ipynb

